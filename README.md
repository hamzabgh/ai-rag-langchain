# ğŸ• Pizza Restaurant RAG Assistant

A professional AI assistant that answers questions about pizza restaurants using customer reviews. Built with LangChain, Ollama LLMs, and Chroma vector database.

## ğŸš€ Features

- **AI-Powered Q&A**: Ask questions about pizza restaurants and get answers based on real reviews
- **Premium Web Interface**: Beautiful Streamlit interface with chat history and analytics
- **Fast Vector Search**: Retrieve relevant reviews using semantic search
- **Local LLM**: Runs completely offline using Ollama models
- **Easy to Use**: Simple setup and intuitive interface

## ğŸ“ Project Structure

```
pizza_rag_project/
â”œâ”€â”€ app/                    # Streamlit web interface
â”œâ”€â”€ config/                # Configuration settings
â”œâ”€â”€ core/                  # Constants and core utilities
â”œâ”€â”€ data/                  # CSV data files
â”œâ”€â”€ database/              # Vector database operations
â”œâ”€â”€ models/                # LLM models and chains
â”œâ”€â”€ chroma_langchain_db/   # Chroma vector database (auto-generated)
â”œâ”€â”€ main.py               # CLI version
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## âš™ï¸ Setup & Installation

### 1. Prerequisites
- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running
- Required Ollama models pulled:
  ```bash
  ollama pull phi          # LLM model
  ollama pull mxbai-embed-large  # Embedding model
  ```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Prepare Data
Place your `realistic_restaurant_reviews.csv` file in the `data/` directory.

## ğŸ® Usage

### Option 1: Web Interface (Recommended)
```bash
streamlit run app/streamlit_app.py
```
Then open `http://localhost:8501` in your browser.

### Option 2: Command Line Interface
```bash
python main.py
```

### Option 3: Launcher
```bash
python run_app.py
```

## ğŸ–¥ï¸ Interface Features

### Web Interface
- **ğŸ’¬ Chat Interface**: Natural conversation with the AI assistant
- **ğŸ“Š Analytics Dashboard**: Visual insights and statistics
- **âš™ï¸ Customizable Settings**: Adjust search parameters and models
- **ğŸ’¡ Sample Questions**: Quick-start with common queries
- **ğŸ“± Responsive Design**: Works on desktop and mobile

### Sample Questions
- "What's the best pizza place in town?"
- "Which restaurant has the best crust?"
- "Where can I find vegetarian pizza options?"
- "Which place has the fastest delivery?"

## ğŸ› ï¸ Configuration

Edit `config/settings.py` to customize:
- LLM model (`phi`, `llama2`, `mistral`, etc.)
- Embedding model
- Search parameters
- File paths

## ğŸ”§ Technical Details

### Built With
- **LangChain**: Framework for LLM applications
- **Ollama**: Local LLM inference
- **Chroma**: Vector database for semantic search
- **Streamlit**: Web interface framework
- **Pandas**: Data manipulation

### How It Works
1. **Data Ingestion**: CSV reviews are loaded and embedded
2. **Vector Storage**: Embeddings stored in Chroma database
3. **Semantic Search**: User questions matched to relevant reviews
4. **LLM Generation**: Phi model generates answers based on retrieved reviews
5. **Response Display**: Answers presented in user-friendly format

## ğŸ“Š Performance
- âš¡ Response time: 2-5 seconds
- ğŸ” Retrieval accuracy: Top 5 most relevant reviews
- ğŸ’¾ Storage: ~100MB for 1000 reviews with embeddings

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ™‹ Support

For issues and questions:
1. Check the [Ollama documentation](https://github.com/ollama/ollama)
2. Review the LangChain docs
3. Open an issue in this repository

---

**Enjoy your pizza research! ğŸ•**